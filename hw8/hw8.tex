\documentclass[a4paper]{article}
\usepackage{tikz}
\usepackage{geometry}
\usepackage{graphicx}
\usepackage{natbib}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsthm}
\usepackage{paralist}
\usepackage{epstopdf}
\usepackage{tabularx}
\usepackage{longtable}
\usepackage{multirow}
\usepackage{multicol}
\usepackage[hidelinks]{hyperref}
\usepackage{fancyvrb}
\usepackage{algorithm}
\usepackage{algorithmic}
\usepackage{float}
\usepackage{paralist}
%\usepackage[svgname]{xcolor}
\usepackage{enumerate}
\usepackage{array}
\usepackage{times}
\usepackage{url}
\usepackage{fancyhdr}
\usepackage{comment}
\usepackage{environ}
\usepackage{times}
\usepackage{textcomp}
\usepackage{caption}
\usepackage{bbm}


\urlstyle{rm}

\setlength\parindent{0pt} % Removes all indentation from paragraphs
\theoremstyle{definition}
\newtheorem{definition}{Definition}[]
\newtheorem{conjecture}{Conjecture}[]
\newtheorem{example}{Example}[]
\newtheorem{theorem}{Theorem}[]
\newtheorem{lemma}{Lemma}
\newtheorem{proposition}{Proposition}
\newtheorem{corollary}{Corollary}

\floatname{algorithm}{Procedure}
\renewcommand{\algorithmicrequire}{\textbf{Input:}}
\renewcommand{\algorithmicensure}{\textbf{Output:}}
\newcommand{\abs}[1]{\lvert#1\rvert}
\newcommand{\norm}[1]{\lVert#1\rVert}
\newcommand{\RR}{\mathbb{R}}
\newcommand{\CC}{\mathbb{C}}
\newcommand{\Nat}{\mathbb{N}}
\newcommand{\br}[1]{\{#1\}}
\DeclareMathOperator*{\argmin}{arg\,min}
\DeclareMathOperator*{\argmax}{arg\,max}
\renewcommand{\qedsymbol}{$\blacksquare$}

\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\definecolor{C0}{HTML}{1F77B4}
\definecolor{C1}{HTML}{FF7F0E}
\definecolor{C2}{HTML}{2ca02c}
\definecolor{C3}{HTML}{d62728}
\definecolor{C4}{HTML}{9467bd}
\definecolor{C5}{HTML}{8c564b}
\definecolor{C6}{HTML}{e377c2}
\definecolor{C7}{HTML}{7F7F7F}
\definecolor{C8}{HTML}{bcbd22}
\definecolor{C9}{HTML}{17BECF}

\newcommand{\Var}{\mathrm{Var}}
\newcommand{\Cov}{\mathrm{Cov}}
\newcommand{\sgn}{\mathrm{sgn}}

\newcommand{\vc}[1]{\boldsymbol{#1}}
\newcommand{\xv}{\vc{x}}
\newcommand{\Sigmav}{\vc{\Sigma}}
\newcommand{\alphav}{\vc{\alpha}}
\newcommand{\muv}{\vc{\mu}}

\newcommand{\red}[1]{\textcolor{red}{#1}}

\def\x{\mathbf x}
\def\y{\mathbf y}
\def\w{\mathbf w}
\def\v{\mathbf v}
\def\E{\mathbb E}
\def\R{\mathbb R}
\def\V{\mathbb V}
\def\ind{\mathbbm 1}

% TO SHOW SOLUTIONS, include following (else comment out):
\newenvironment{soln}{
    \leavevmode\color{blue}\ignorespaces
}{}


\hypersetup{
%    colorlinks,
    linkcolor={red!50!black},
    citecolor={blue!50!black},
    urlcolor={blue!80!black}
}

\geometry{
  top=1in,            % <-- you want to adjust this
  inner=1in,
  outer=1in,
  bottom=1in,
  headheight=3em,       % <-- and this
  headsep=2em,          % <-- and this
  footskip=3em,
}


\pagestyle{fancyplain}
\lhead{\fancyplain{}{Homework 8}}
\rhead{\fancyplain{}{CS 760 Machine Learning}}
\cfoot{\thepage}

\title{\textsc{Homework 8}} % Title

%%% NOTE:  Replace 'NAME HERE' etc., and delete any "\red{}" wrappers (so it won't show up as red)

\author{
$>>$Sean Sun$<<$ \\
$>>$9078202463$<<$\\
} 

\date{}

\begin{document}

\maketitle 


\textbf{Instructions:} 
Although this is a programming homework, you only need to hand in a pdf answer file.
There is no need to submit the latex source or any code.
You can choose any programming language, as long as you implement the algorithm from scratch.

Use this latex file as a template to develop your homework.
Submit your homework on time as a single pdf file to Canvas.
Please check Piazza for updates about the homework.


\section{Directed Graphical Model [20 points]}
Consider the directed graphical model (aka Bayesian network) in Figure~\ref{fig:bn}.
\begin{figure}[H]
        \centering
                \includegraphics[width=0.8\textwidth]{BN.jpg}
        \caption{A Bayesian Network example.}
        \label{fig:bn}
\end{figure}
Compute $P(B=t \mid E=f,J=t,M=t)$ and $P(B=t \mid E=t,J=t,M=t)$.
These are the conditional probabilities of a burglar in your house (yikes!) when both of your neighbors John and Mary call you and say they hear an alarm in your house, but without or with an earthquake also going on in that area (what a busy day), respectively.

\begin{soln}

$P(B=t \mid E=f,J=t,M=t)\\
 = \frac{P(B=t, E=f,J=t,M=t)}{P(E=f,J=t,M=t) }
 = 
\frac{\sum_A {P(B=t)P(E=f)P(M=t \mid A)P(J=t \mid A)P(A \mid B=t, E=f)}}
{\sum_A {\sum_B P(B)P(E=f)P(M=t \mid A)P(J=t \mid A)P(A \mid B, E=f)}}
= \frac{0.04064}{0.05832} 
\\= 0.4106 $\\\\
$P(B=t \mid E=t,J=t,M=t) 
\\= \frac{P(B=t, E=t,J=t,M=t)}{P(E=t,J=t,M=t)}
 = 
\frac{\sum_A {P(B=t)P(E=t)P(M=t \mid A)P(J=t \mid A)P(A \mid B=t, E=t)}}
{\sum_A {\sum_B P(B)P(E=t)P(M=t \mid A)P(J=t \mid A)P(A \mid B, E=t)}}= \frac{0.01138}{0.03654}
\\=0.2375
 $
\end{soln}

\section{Chow-Liu Algorithm [20 pts]}
Suppose we wish to construct a directed graphical model for 3 features $X$, $Y$, and $Z$ using the Chow-Liu algorithm. We are given data from 100 independent experiments where each feature is binary and takes value $T$ or $F$. Below is a table summarizing the observations of the experiment:

\begin{table}[H]
        \centering
                \begin{tabular}{cccc}
                           $X$ & $Y$ & $Z$ & Count \\
                                \hline
                                T & T & T & 36 \\
                                \hline
                                T & T & F & 4 \\
                                \hline
                                T & F & T & 2 \\
                                \hline
                                T & F & F & 8 \\
                                \hline
                                F & T & T & 9 \\
                                \hline
                                F & T & F & 1 \\
                                \hline
                                F & F & T & 8 \\
                                \hline
                                F & F & F & 32 \\
                                \hline
                \end{tabular}
\end{table}

\begin{enumerate}
\item Compute the mutual information $I(X, Y)$ based on the frequencies observed in the data.

\begin{soln}

$I(X, Y) = \sum_{(x,y)}\hat{P}(X=x,Y=y)log\frac{\hat{P}(X=x,Y=y)}{\hat{P}(X=x)\hat{P}(Y=y)} = 0.2781$
\end{soln}

\item Compute the mutual information $I(X, Z)$ based on the frequencies observed in the data.
\begin{soln}

$I(X, Z) = \sum_{(x,z)}\hat{P}(X=x,Z=z)log\frac{\hat{P}(X=x,Z=z)}{\hat{P}(X=x)\hat{P}(Z=z)} = 0.1328$
\end{soln}
\item Compute the mutual information $I(Z, Y)$ based on the frequencies observed in the data.
\begin{soln}

$I(Z, Y) = \sum_{(z,y)}\hat{P}(Z=z,Y=y)log\frac{\hat{P}(Z=z,Y=y)}{\hat{P}(Z=z)\hat{P}(Y=y)} = 0.3973$
\end{soln}
\item Which undirected edges will be selected by the Chow-Liu algorithm as the maximum spanning tree?
\begin{soln}

We should choose E(Z,Y) and E(X,Y) according to greedy algotirhm
\end{soln}
\item Root your tree at node $X$, assign directions to the selected edges.
\begin{soln}
\begin{center}
\includegraphics[width=0.3\textwidth]{q25.png}
\end{center}
\end{soln}

\end{enumerate}


\section{Kernel SVM [20 points]}
Consider the following kernel function defined over $z,z'\in Z$:
\begin{align*}
k(z,z') =
\begin{cases}
1 & \text{~if~} z=z', \\
0 & \text{~otherwise.}
\end{cases}
\end{align*}
\begin{enumerate}
\item Prove that for any integer $m>0$, any $z_1, \ldots, z_m \in Z$, the $m \times m$ kernel matrix $K=[K_{ij}]$ is positive semi-definite, where $K_{ij}=k(z_i, z_j)$ for $i,j=1\ldots m$.
Hint: An $m\times m$ matrix $K$ is positive semi-definite if $\forall v \in \R^m, v^\top K v \ge 0$.
\begin{soln}

$K$ is an identity matrix, i.e.$ K = I_{m}$\\
So, $\forall v \in \R^m, v^\top K v = v^\top I_{m} v =  v^\top v \ge 0$\\
So, kernel matrix $K=[K_{ij}]$ is positive semi-definite
\end{soln}

\item Given a training set $(z_1, y_1), \ldots, (z_n, y_n)$ with binary labels, the dual SVM problem with the above kernel $k$ will have parameters $\alpha_1, \ldots, \alpha_n, b \in \R$.  The predictor for input $z$ takes the form
$$f(z) = \sum_{i=1}^n \alpha_i y_i k(z_i, z) + b.$$
Recall the label prediction is $\sgn(f(z))$.
Prove that there exists $\alpha_1, \ldots, \alpha_n, b$ such that $f$ correctly separates the training set.
In other words, $k$ induces a feature space rich enough such that in it any training set can be linearly separated.
\begin{soln}
\\When a same $z$ can have different $y$ values, the bayes error will beb greater than zero, which means it is by no means we can have a set of parameters to linearly separate the training set. So, for this question, I assume all $z$ values are distinct or same "z" values will have same $y$ value.
\\\\
For every $z_{j}$ in training set, $ \sum_{i=1}^n \alpha_i y_i k(z_i, z) =  \alpha_j y_j k(z_j, z_j) = \alpha_j y_j$ since all other $k(z_i, z_j)$ are zeros.\\
So, $f(z_{j}) = \alpha_j y_j + b$\\
When we set all $\alpha$ to be 1 and set b to be 0, $f(z_{j}) = y_j$, which means we can linearly separate all training set points.\\
So, there exists $\alpha_1, \ldots, \alpha_n, b$ such that $f$ correctly separates the training set.
\end{soln}


\item How does that $f$ predict input $z$ that is not in the training set?

\begin{soln}
All $z$ that is not in training set will be labeled as 0
\end{soln}

\end{enumerate}

Comment: One useful property of kernel functions is that the input space $Z$ does not need to be a vector space; in other words, $z$ does not need to be a feature vector.  For all we know, $Z$ can be turkeys in the world.  As long as we can compute $k(z,z')$, kernel SVM works on turkeys.

\section{Principal Component Analysis [40 pts]}
Download three.txt and eight.txt.  Each has 200 handwritten digits.  Each line is for a digit, vectorized from a 16x16 gray scale image.  
\begin{enumerate}
\item (5 pts) Each line has 256 numbers: they are pixel values (0=black, 255=white) vectorized from the image as the first column (top down), the second column, and so on.
Visualize the two gray scale images corresponding to the first line in three.txt and the first line in eight.txt.

\begin{soln}
\includegraphics[width=0.5\textwidth]{three.png}
\includegraphics[width=0.5\textwidth]{eight.png}
\end{soln}

\item (5 pts) Putting the two data files together (threes first, eights next) to form a $n \times D$ matrix $X$ where $n=400$ digits and $D=256$ pixels.  Note we use $n\times D$ size for $X$ instead of $D\times n$ to be consistent with the convention in linear regression.   The $i$th row of $X$ is $x_i^\top$, where $x_i \in \R^D$ is the $i$th image in the combined data set.
Compute the sample mean $y = {1\over n} \sum_{i=1}^n x_i$.
Visualize $y$ as a 16x16 gray scale image.

\begin{soln}
\begin{center}
\includegraphics[width=0.5\textwidth]{q42.png}
\end{center}
\end{soln}

\item (10 pts) Center $X$ using $y$ above.  Then form the sample covariance matrix $S={X^\top X \over n-1}$.
Show the 5x5 submatrix $S(1\ldots 5, 1 \ldots 5)$.
\begin{soln}
\begin{center}
$
\begin{bmatrix}	
59.17  & 142.15  & 28.68  & -7.18  & -14.34  & \\
142.15  & 878.94  & 374.14  & 24.13  & -87.13  & \\
28.68  & 374.14  & 1082.91  & 555.23  & 33.72  & \\
-7.18  & 24.13  & 555.23  & 1181.24  & 777.77  & \\
-14.34  & -87.13  & 33.72  & 777.77  & 1429.96  & \\
	\end{bmatrix}
$
\end{center}
\end{soln}

\item (10 pts) Use appropriate software to compute the two largest eigenvalues $\lambda_1 \ge \lambda_2$ and the corresponding eigenvectors $v_1, v_2$ of $S$.
For example, in Matlab one can use eigs(S,2).  
Show the value of $\lambda_1, \lambda_2$.
Visualize $v_1, v_2$ as two 16x16 gray scale images.
Hint: their elements will not be in [0, 255], but you can shift and scale them appropriately.  It is best if you can show an accompany ``colorbar'' that maps gray scale to values. 

\begin{soln}

 $\lambda_1 =237155.2463\\
 \lambda_2 = 145188.3527$\\
\includegraphics[width=0.4\textwidth]{eigenv1.png}
\includegraphics[width=0.4\textwidth]{eigenv2.png}
\end{soln}
\item (5 pts) Now we project (the centered) $X$ down to the two PCA directions.   Let $V=[v_1 v_2]$ be the $D\times 2$ matrix.  The projection is simply $XV$.
Show the resulting two coordinates for the first line in three.txt and the first line in eight.txt, respectively.

\begin{soln}
first line in three: ( -109.36 , 77.52 )\\
first line in eight: ( 94.08 , 126.88 )
\end{soln}

\item (5 pts) Now plot the 2D point cloud of the 400 digits after projection.
For visual interest, color points in three.txt red and points in eight.txt blue.
But keep in mind that PCA is an unsupervised learning method and it does not know such class labels.

\begin{soln}
\begin{center}
\includegraphics[width=0.6\textwidth]{q46.png}
\end{center}
\end{soln}
\end{enumerate}

\end{document}
